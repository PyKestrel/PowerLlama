# PowerLlama

Ollama API Wrapper In PowerShell

# New-OllamaCompletion

## SYNOPSIS

Generate a response for a given prompt with a provided model.

## DESCRIPTION

Generate a response for a given prompt with a provided model. This is a streaming endpoint, so will be a series of responses. The final response object will include statistics and additional data from the request.

## PARAMETERS

- **URI**: Properly Formatted URI Like "https://{IP OR DOMAIN_NAME}:{PORT}"
- **Model**: The Model Name As A String Like "llama2"
- **Prompt**: The Prompt To Generate A Response For
- **Format**: the format to return a response in. Currently the only accepted value is json
- **Options**: (Optional)
- **System**: (Optional)
- **Template**: (Optional)
- **Context**: (Optional)
- **Stream**: If "false", the response will not be streamed.
- **Raw**: If "true", raw content will be included in the response.

## EXAMPLE

```powershell
New-OllamaCompletion -URI "http://10.1.0.1:11434" -Model "llama2" -Format json -Prompt "Why is the sky blue?" -Stream "false"
```

# New-OllamaModel

## SYNOPSIS

Create a model from a Modelfile.

## PARAMETERS

- **URI**: Properly Formatted URI Like "https://{IP OR DOMAIN_NAME}:{PORT}"
- **Name**: The name to be given to the model as a string like "mario"
- **Path**: Path to the model file on the remote system "~/Modelfile"
- **Stream**: If "false", the response will be returned as a single response object, rather than a stream of objects.

## EXAMPLE

```powershell
New-OllamaModel -URI "http://10.1.0.1:11434" -Name "mario" -Path "~/Modelfile" -Stream "false"
```

# Get-OllamaModels

## SYNOPSIS

Get the list of available models.

## PARAMETERS

- **URI**: Properly Formatted URI Like "https://{IP OR DOMAIN_NAME}:{PORT}"

## EXAMPLE

```powershell
Get-OllamaModels -URI "http://10.1.0.1:11434"
```

# Show-OllamaModel

## SYNOPSIS

Get details about a model, including its Modelfile, Template, Parameters, License & System Prompt.

## PARAMETERS

- **URI**: Properly Formatted URI Like "https://{IP OR DOMAIN_NAME}:{PORT}"
- **Name**: The Name Of The Model As A String Like "mario"

## EXAMPLE

```powershell
Show-OllamaModel -URI "http://10.1.0.1:11434" -Name "mario"
```

# Copy-OllamaModel

## SYNOPSIS

Copy a model by creating another from an existing model.

## PARAMETERS

- **URI**: Properly Formatted URI Like "https://{IP OR DOMAIN_NAME}:{PORT}"
- **Source**: Source Name Of The Model As A String Like "mario"
- **Destination**: Destination Name Of The Model As A String Like "mario2"

## EXAMPLE

```powershell
Copy-OllamaModel -URI "http://10.1.0.1:11434" -Source "mario" -Destination "mario2"
```

# Remove-OllamaModel

## SYNOPSIS

Delete an existing model.

## PARAMETERS

- **URI**: Properly Formatted URI Like "https://{IP OR DOMAIN_NAME}:{PORT}"
- **Name**: Name Of The Model As A String Like "mario"

## EXAMPLE

```powershell
Remove-OllamaModel -URI "http://10.1.0.1:11434" -Name "mario"
```

# Install-OllamaModel

## SYNOPSIS

Download a model from the Ollama Library.

## PARAMETERS

- **Name**: Name of the Model in the following format "<namespace>/<model>:<tag>"
- **AllowInsecure**: Allow insecure connections ("true" or "false")
- **Stream**: If "false", the response will be returned as a single response object, rather than a stream of objects.

## EXAMPLE

```powershell
Install-OllamaModel -Name "namespace/mario:latest" -AllowInsecure "false" -Stream "true"
```

# New-OllamaModel

## SYNOPSIS

Upload a model to the Ollama Library. (Requires registering for ollama.ai and adding a public key first.)

## PARAMETERS

- **Name**: Name of the Model as a string like "mario"
- **AllowInsecure**: Allow insecure connections ("true" or "false")
- **Stream**: If "false", the response will be returned as a single response object, rather than a stream of objects.

## EXAMPLE

```powershell
New-OllamaModel -Name "mario" -AllowInsecure "false" -Stream "true"
```
